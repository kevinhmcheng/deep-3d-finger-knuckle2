{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "class FNDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, train=True):\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_labels.iloc[idx, 0]\n",
    "        image = Image.open(os.path.join(self.img_dir, img_path)).convert('RGB')\n",
    "        if not self.train: #for test\n",
    "            image = image.crop((2, 3, 100-2, 70-3))\n",
    "        \n",
    "        \n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        \n",
    "        if self.transform:\n",
    "            transform = []\n",
    "            transform.append(T.ToTensor())\n",
    "            transform.append(T.Normalize([0.5]*3, [0.5]*3))\n",
    "            transform = T.Compose(transform)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = torch.squeeze(model(X))\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch+1 == len(dataloader):\n",
    "            loss, current, total_it = loss.item(), batch+1, len(dataloader)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{total_it:>5d}]\")\n",
    "            \n",
    "def train2m(dataloader, model, model2, loss_fn, optimizer, optimizer2):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    model2.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = torch.squeeze(model2(model(X)))\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "        if batch+1 == len(dataloader):\n",
    "            loss, current, total_it = loss.item(), batch+1, len(dataloader)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{total_it:>5d}]\")\n",
    "\n",
    "import scipy.io\n",
    "def test(dataloader, model, loss_fn, save=False, batch_size=64):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    count = 0;\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = X[:,:,0+8:64-8,0+8:96-8] #center\n",
    "            \n",
    "            pred = model(X)\n",
    "            m = nn.Softmax(dim=1)\n",
    "            pred = m(pred)\n",
    "            \n",
    "            actual = y\n",
    "            \n",
    "            test_loss += loss_fn(pred, actual).item()   \n",
    "            correct += (pred.argmax(1) == actual).type(torch.float).sum().item()\n",
    "            \n",
    "            if save:\n",
    "                if count == 0:\n",
    "                    Pred = pred\n",
    "                    Actual = actual\n",
    "                else:\n",
    "                    Pred = torch.cat((Pred, pred), 0)\n",
    "                    Actual = torch.cat((Actual, actual), 0)\n",
    "                count = count + 1\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    if save:\n",
    "        scipy.io.savemat(out_path+'scores-'+str(batch_size)+'.mat',{'Pred':Pred.cpu().numpy(),'Actual':Actual.cpu().numpy(),'Accuracy':correct,'Loss':test_loss})\n",
    "        \n",
    "\n",
    "def test_align(dataloader, model, loss_fn, center=False, save=False, batch_size=64):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    count = 0;\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            X_this = torch.zeros(X.shape).to(device)\n",
    "            pred_this = torch.zeros([X.size(dim=0),190,8,8]).to(device)\n",
    "            for shifti in range(1,8+1):\n",
    "                for shiftj in range(1,8+1):\n",
    "                    X_this[:,:,0:64-shifti,0:96-shiftj] = X[:,:,0+shifti:64,0+shiftj:96]\n",
    "                    X_this[:,:,64-shifti:64,96-shiftj:96] = X[:,:,0:shifti,0:shiftj]\n",
    "                    X_this[:,:,64-shifti:64,0:96-shiftj] = X[:,:,0:shifti,0+shiftj:96]\n",
    "                    X_this[:,:,0:64-shifti,96-shiftj:96] = X[:,:,0+shifti:64,0:shiftj]\n",
    "                    \n",
    "                    if center:\n",
    "                        X_in = X_this[:,:,0+8:64-8,0+8:96-8] #center\n",
    "                    else:\n",
    "                        X_in = X_this\n",
    "                    \n",
    "                    pred = model(X_in)\n",
    "                    m = nn.Softmax(dim=1)\n",
    "                    pred = m(pred)\n",
    "                    \n",
    "                    \n",
    "                    if pred.dim() == 4:\n",
    "                        pred = pred.reshape(pred.size(0),pred.size(1),pred.size(2)*pred.size(3))\n",
    "                        temp, _ = torch.max(pred,1)\n",
    "                        _, idx = torch.max(temp,1)\n",
    "                        pred = pred[:,:,idx].reshape(pred.size(0),pred.size(1))\n",
    "                    pred_this[:,:,shifti-1,shiftj-1] = pred\n",
    "            \n",
    "            pred_this = pred_this.reshape(pred_this.size(0),pred_this.size(1),pred_this.size(2)*pred_this.size(3))\n",
    "            temp, _ = torch.max(pred_this,1)\n",
    "            _, idx = torch.max(temp,1)\n",
    "            pred = pred_this[:,:,idx].reshape(pred_this.size(0),pred_this.size(1))\n",
    "            actual = y\n",
    "            \n",
    "            test_loss += loss_fn(pred, actual).item()   \n",
    "            correct += (pred.argmax(1) == actual).type(torch.float).sum().item()\n",
    "            if save:\n",
    "                if count == 0:\n",
    "                    Pred = pred\n",
    "                    Actual = actual\n",
    "                else:\n",
    "                    Pred = torch.cat((Pred, pred), 0)\n",
    "                    Actual = torch.cat((Actual, actual), 0)\n",
    "                count = count + 1\n",
    "                \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if save:\n",
    "        scipy.io.savemat(out_path+'scores-align-'+str(batch_size)+'.mat',{'Pred':Pred.cpu().numpy(),'Actual':Actual.cpu().numpy(),'Accuracy':correct,'Loss':test_loss})\n",
    "    \n",
    "            \n",
    "def test_align2m(dataloader, model, model2, loss_fn, center=False, save=False, batch_size=64):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    model2.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    count = 0;\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            X_this = torch.zeros(X.shape).to(device)\n",
    "            pred_this = torch.zeros([X.size(dim=0),190,8,8]).to(device)\n",
    "            for shifti in range(1,8+1):\n",
    "                for shiftj in range(1,8+1):\n",
    "                    X_this[:,:,0:64-shifti,0:96-shiftj] = X[:,:,0+shifti:64,0+shiftj:96]\n",
    "                    X_this[:,:,64-shifti:64,96-shiftj:96] = X[:,:,0:shifti,0:shiftj]\n",
    "                    X_this[:,:,64-shifti:64,0:96-shiftj] = X[:,:,0:shifti,0+shiftj:96]\n",
    "                    X_this[:,:,0:64-shifti,96-shiftj:96] = X[:,:,0+shifti:64,0:shiftj]\n",
    "                    \n",
    "                    if center:\n",
    "                        X_in = X_this[:,:,0+8:64-8,0+8:96-8] #center\n",
    "                    else:\n",
    "                        X_in = X_this\n",
    "                    \n",
    "                    pred = model2(model(X_in))\n",
    "                    m = nn.Softmax(dim=1)\n",
    "                    pred = m(pred)\n",
    "                    \n",
    "                    if pred.dim() == 4:\n",
    "                        pred = pred.reshape(pred.size(0),pred.size(1),pred.size(2)*pred.size(3))\n",
    "                        temp, _ = torch.max(pred,1)\n",
    "                        _, idx = torch.max(temp,1)\n",
    "                        pred = pred[:,:,idx].reshape(pred.size(0),pred.size(1))\n",
    "                    pred_this[:,:,shifti-1,shiftj-1] = pred\n",
    "            \n",
    "            pred_this = pred_this.reshape(pred_this.size(0),pred_this.size(1),pred_this.size(2)*pred_this.size(3)) \n",
    "            temp, _ = torch.max(pred_this,1)\n",
    "            _, idx = torch.max(temp,1)\n",
    "            pred = pred_this[:,:,idx].reshape(pred_this.size(0),pred_this.size(1))\n",
    "            actual = y\n",
    "            \n",
    "            test_loss += loss_fn(pred, actual).item()   \n",
    "            correct += (pred.argmax(1) == actual).type(torch.float).sum().item()\n",
    "            if save:\n",
    "                if count == 0:\n",
    "                    Pred = pred\n",
    "                    Actual = actual\n",
    "                else:\n",
    "                    Pred = torch.cat((Pred, pred), 0)\n",
    "                    Actual = torch.cat((Actual, actual), 0)\n",
    "                count = count + 1\n",
    "                \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if save:\n",
    "        scipy.io.savemat(out_path+'scores-align-'+str(batch_size)+'.mat',{'Pred':Pred.cpu().numpy(),'Actual':Actual.cpu().numpy(),'Accuracy':correct,'Loss':test_loss})\n",
    "    \n",
    "            \n",
    "def test_align_match(train_dataloader, test_dataloader, model, center=False, save=False, batch_size=64, sim=1):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        count = 0;\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            actual = y\n",
    "            if save:\n",
    "                if count == 0:\n",
    "                    Pred = pred\n",
    "                    Actual = actual\n",
    "                else:\n",
    "                    Pred = torch.cat((Pred, pred), 0)\n",
    "                    Actual = torch.cat((Actual, actual), 0)\n",
    "                count = count + 1\n",
    "        Pred_train = torch.squeeze(Pred)\n",
    "\n",
    "    \n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "        \n",
    "        test_loss, correct = 0, 0\n",
    "        count = 0;\n",
    "        for i, (X, y) in enumerate(test_dataloader):\n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            X_this = torch.zeros(X.shape).to(device)\n",
    "            dist_this = torch.zeros([X.size(dim=0),Pred_train.size(dim=0),8,8]).to(device)\n",
    "            for shifti in range(1,8+1):\n",
    "                for shiftj in range(1,8+1):\n",
    "                    X_this[:,:,0:64-shifti,0:96-shiftj] = X[:,:,0+shifti:64,0+shiftj:96]\n",
    "                    X_this[:,:,64-shifti:64,96-shiftj:96] = X[:,:,0:shifti,0:shiftj]\n",
    "                    X_this[:,:,64-shifti:64,0:96-shiftj] = X[:,:,0:shifti,0+shiftj:96]\n",
    "                    X_this[:,:,0:64-shifti,96-shiftj:96] = X[:,:,0+shifti:64,0:shiftj]\n",
    "                    \n",
    "                    if center:\n",
    "                        X_in = X_this[:,:,0+8:64-8,0+8:96-8] #center\n",
    "                        pred = model(X_in)\n",
    "                        temp1 = Pred_train.repeat(pred.size(dim=0),1,1)\n",
    "                        temp2 = torch.squeeze(pred).repeat(Pred_train.size(dim=0),1,1).permute((1,0,2))\n",
    "                        if sim == 1:\n",
    "                            temp3 = -torch.square(temp1-temp2)\n",
    "                            temp4 = torch.mean(temp3, 2)\n",
    "                        else:\n",
    "                            cos = nn.CosineSimilarity(dim=2)\n",
    "                            temp4 = cos(temp1, temp2)\n",
    "                        dist = temp4\n",
    "                    else:\n",
    "                        X_in = X_this\n",
    "\n",
    "                        pred = model(X_in)\n",
    "                        dist = torch.zeros([X.size(dim=0),Pred_train.size(dim=0),3,3]).to(device)\n",
    "                        for shifti3 in range(0,3):\n",
    "                            for shiftj3 in range(0,3):\n",
    "                                temp1 = Pred_train.repeat(pred.size(dim=0),1,1)\n",
    "                                temp2 = torch.squeeze(pred[:,:,shifti3,shiftj3]).repeat(Pred_train.size(dim=0),1,1).permute((1,0,2)) #190, batch_size,1000\n",
    "                                if sim == 1:\n",
    "                                    temp3 = -torch.square(temp1-temp2)\n",
    "                                    temp4 = torch.mean(temp3, 2)\n",
    "                                else:\n",
    "                                    cos = nn.CosineSimilarity(dim=2)\n",
    "                                    temp4 = cos(temp1, temp2)\n",
    "                                dist[:,:,shifti3,shiftj3] = temp4\n",
    "                        dist, _ = torch.max(dist,3)\n",
    "                        dist, _ = torch.max(dist,2)\n",
    "\n",
    "                    dist_this[:,:,shifti-1,shiftj-1] = dist\n",
    "            \n",
    "            dist, _ = torch.max(dist_this,3)\n",
    "            dist, _ = torch.max(dist,2)\n",
    "            actual = y\n",
    "            \n",
    "            test_loss += torch.mean(dist).item()   \n",
    "            correct += (dist.argmax(1) == actual).type(torch.float).sum().item()\n",
    "            if save:\n",
    "                if count == 0:\n",
    "                    Pred = dist\n",
    "                    Actual = actual\n",
    "                else:\n",
    "                    Pred = torch.cat((Pred, dist), 0)\n",
    "                    Actual = torch.cat((Actual, actual), 0)\n",
    "                count = count + 1\n",
    "                \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if save:\n",
    "        if sim == 1:\n",
    "            scipy.io.savemat(out_path+'scores-align-match-'+str(batch_size)+'.mat',{'Pred':Pred.cpu().numpy(),'Actual':Actual.cpu().numpy(),'Accuracy':correct,'Loss':test_loss})\n",
    "        else:\n",
    "            scipy.io.savemat(out_path+'scores-align-match-cos-'+str(batch_size)+'.mat',{'Pred':Pred.cpu().numpy(),'Actual':Actual.cpu().numpy(),'Accuracy':correct,'Loss':test_loss})\n",
    "    \n",
    "\n",
    "def denorm(x):\n",
    "        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1\n",
      "3990\n",
      "1140\n",
      "Feature batch shape: torch.Size([64, 3, 48, 80])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 109\n",
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.768933 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.063949 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.067245 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.027075 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.022946 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.017404 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.018705 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.023785 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.015927 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.019690 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.009679 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.015629 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.023783 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.011369 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.012324 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.010486 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.013917 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.009090 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.007999 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.007261 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.019756 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.016287 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.008366 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.005983 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.005884 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.008832 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.006779 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.008606 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.012314 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.011733 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.011348 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.012296 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.005370 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.008439 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.011582 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.006528 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.008808 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.006481 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.008710 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.011488 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.008994 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.011453 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.013436 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.006292 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.005685 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.010260 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.005571 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.004396 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.009534 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.006960 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Done!\n",
      "Saved PyTorch Model State\n",
      "Model: 2\n",
      "3990\n",
      "1140\n",
      "Feature batch shape: torch.Size([64, 3, 48, 80])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n",
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.107018 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.014671 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.010506 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.007039 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.005864 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.006146 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.005324 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.003435 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.004386 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.003752 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.004369 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.003004 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.002620 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.001930 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.002096 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.002066 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.002231 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.002041 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.002219 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.002135 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.001223 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.002821 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.001765 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.001733 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.002243 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.002028 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.001317 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.001974 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.001347 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.002028 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.002483 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.001121 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.002121 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.002401 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.002353 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.001750 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.001055 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.001524 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.002570 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.001966 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.002113 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.002095 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.001630 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.001406 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.001068 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.001444 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.002091 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.001264 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.002133 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.001538 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Done!\n",
      "Saved PyTorch Model State\n",
      "Model: 3\n",
      "3990\n",
      "1140\n",
      "Feature batch shape: torch.Size([64, 3, 48, 80])\n",
      "Label batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 21\n",
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.093135 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.034341 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.026378 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.018601 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.015669 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.012861 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.012091 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.012813 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.013804 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.010677 [   63/   63]\n",
      "Learning Rate [0.005]\n",
      "-------------------------------\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.007944 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.008694 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.007330 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.007998 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.007395 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.006756 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.006498 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.007177 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.005893 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.006127 [   63/   63]\n",
      "Learning Rate [0.0025]\n",
      "-------------------------------\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.005665 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.005587 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.005643 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.006282 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.006135 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.009204 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.007474 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.005474 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.004736 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.007069 [   63/   63]\n",
      "Learning Rate [0.00125]\n",
      "-------------------------------\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.005349 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.005113 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.004455 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.005188 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.006183 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.005730 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.004652 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.006172 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.005671 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.005584 [   63/   63]\n",
      "Learning Rate [0.000625]\n",
      "-------------------------------\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.004759 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.005224 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.004302 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.006281 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.005121 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.005310 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.005296 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.004917 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.005172 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.004759 [   63/   63]\n",
      "Learning Rate [0.0003125]\n",
      "-------------------------------\n",
      "Done!\n",
      "Saved PyTorch Model State\n"
     ]
    }
   ],
   "source": [
    "for model_select in range(1,3+1):\n",
    "    print('Model: ' + str(model_select))\n",
    "\n",
    "    train_label_filename = 'train_label_shuffle.txt'\n",
    "    test_label_filename = 'test_label.txt'\n",
    "\n",
    "    data_path = \"data_label_190/\"\n",
    "    out_path = 'task/' + 'model' + str(model_select) + '/'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    train_data = FNDataset(\n",
    "        annotations_file = train_label_filename,\n",
    "        img_dir = data_path,\n",
    "        transform=True,\n",
    "        train=True,\n",
    "    )\n",
    "    test_data = FNDataset(\n",
    "        annotations_file = test_label_filename,\n",
    "        img_dir = data_path,\n",
    "        transform=True,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    batch_size = 64 #64\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(len(train_dataloader.dataset))\n",
    "    print(len(test_dataloader.dataset))\n",
    "\n",
    "    # Display image and label.\n",
    "    \n",
    "    X, y = next(iter(train_dataloader))\n",
    "    print(f\"Feature batch shape: {X.size()}\")\n",
    "    print(f\"Label batch shape: {y.size()}\")\n",
    "    img = X[0].squeeze().permute(1,2,0)\n",
    "    label = y[0]\n",
    "\n",
    "    img = denorm(img)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    print(f\"Label: {label}\")\n",
    "    \n",
    "    \n",
    "    # Get cpu or gpu device for training.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "    \n",
    "\n",
    "\n",
    "    #Model Selection\n",
    "    pretrain = True\n",
    "    if model_select == 1:\n",
    "        model = models.resnet50(pretrained=pretrain)\n",
    "        model.fc = nn.Linear(2048, 190)\n",
    "    elif model_select == 2:\n",
    "        import fknet\n",
    "        model = fknet.resnet50(pretrained=pretrain)\n",
    "        model.fc = nn.Conv2d(832, 190, kernel_size=(6,10), stride=1, padding=0)\n",
    "    elif model_select == 3:\n",
    "        fea_size = 800\n",
    "        import fknet\n",
    "        model = fknet.resnet50(pretrained=pretrain)\n",
    "        model.fc = nn.Sequential(nn.Conv2d(832, fea_size, kernel_size=(6,10), stride=1, padding=0),\n",
    "                                 nn.ReLU(),nn.BatchNorm2d(fea_size) )\n",
    "        model2 = nn.Conv2d(fea_size, 190, kernel_size=1, stride=1, padding=0)\n",
    "        model2 = model2.to(device)\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    epochs = 50\n",
    "    if model_select == 3:\n",
    "        optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.005, momentum=0.9)\n",
    "        scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=10, gamma=0.5)\n",
    "\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        if model_select == 1:\n",
    "            train(train_dataloader, model, loss_fn, optimizer)\n",
    "            #test(test_dataloader, model, loss_fn, save=False, batch_size=batch_size)\n",
    "            #test_align(test_dataloader, model, loss_fn, center=True, save=False, batch_size=batch_size)\n",
    "        elif model_select == 2:\n",
    "            train(train_dataloader, model, loss_fn, optimizer)\n",
    "            #test_align(test_dataloader, model, loss_fn, center=False, save=False, batch_size=batch_size)\n",
    "        elif model_select == 3:\n",
    "            train2m(train_dataloader, model, model2, loss_fn, optimizer, optimizer2)\n",
    "            #test_align2m(test_dataloader, model, model2, loss_fn, center=False, save=False, batch_size=batch_size)\n",
    "            scheduler2.step()\n",
    "\n",
    "        print(f\"Learning Rate {scheduler.get_last_lr()}\\n-------------------------------\")\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Done!\")\n",
    "    torch.save(model.state_dict(), os.path.join(out_path, 'model.pth'))\n",
    "    if model_select == 3:\n",
    "        torch.save(model2.state_dict(), os.path.join(out_path, 'model2.pth'))\n",
    "    print(\"Saved PyTorch Model State\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:1\n",
      "Model: 1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Test Error: \n",
      " Accuracy: 47.0%, Avg loss: 4.888227 \n",
      "\n",
      "Model: 2\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 4.450680 \n",
      "\n",
      "Model: 3\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 4.581877 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification Performance\n",
    "for batch_size in [1]: #current implementation of test_align only supports batch_size = 1\n",
    "    print('batch_size:' + str(batch_size))\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for model_select in range(1,3+1):\n",
    "        print('Model: ' + str(model_select))\n",
    "        out_path = 'task/' + 'model' + str(model_select) + '/'\n",
    "        \n",
    "        #Model Selection\n",
    "        pretrain = True\n",
    "        if model_select == 1:\n",
    "            model = models.resnet50(pretrained=pretrain)\n",
    "            model.fc = nn.Linear(2048, 190)\n",
    "        elif model_select == 2:\n",
    "            import fknet\n",
    "            model = fknet.resnet50(pretrained=pretrain)\n",
    "            model.fc = nn.Conv2d(832, 190, kernel_size=(6,10), stride=1, padding=0)\n",
    "        elif model_select == 3:\n",
    "            fea_size = 800\n",
    "            import fknet\n",
    "            model = fknet.resnet50(pretrained=pretrain)\n",
    "            model.fc = nn.Sequential(nn.Conv2d(832, fea_size, kernel_size=(6,10), stride=1, padding=0),\n",
    "                                     nn.ReLU(),nn.BatchNorm2d(fea_size) )\n",
    "            model2 = nn.Conv2d(fea_size, 190, kernel_size=1, stride=1, padding=0)\n",
    "            model2 = model2.to(device)\n",
    "        model = model.to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        if model_select == 1:\n",
    "            model.load_state_dict(torch.load(os.path.join(out_path, 'model.pth')))\n",
    "            test_align(test_dataloader, model, loss_fn, center=True, save=True, batch_size=batch_size)\n",
    "        elif model_select == 2:\n",
    "            model.load_state_dict(torch.load(os.path.join(out_path, 'model.pth')))\n",
    "            test_align(test_dataloader, model, loss_fn, center=False, save=True, batch_size=batch_size)\n",
    "        elif model_select == 3:\n",
    "            model.load_state_dict(torch.load(os.path.join(out_path, 'model.pth')))\n",
    "            model2.load_state_dict(torch.load(os.path.join(out_path, 'model2.pth')))\n",
    "            test_align2m(test_dataloader, model, model2, loss_fn, center=False, save=True, batch_size=batch_size)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:1\n",
      "Model: 2\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: -0.093124 \n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.672603 \n",
      "\n",
      "Model: 3\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: -0.002676 \n",
      "\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.859268 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feature Matching Performance\n",
    "for batch_size in [1]: \n",
    "    print('batch_size:' + str(batch_size))\n",
    "    train_data_set1 = FNDataset(\n",
    "        annotations_file = 'train_label_set1.txt',\n",
    "        img_dir = data_path,\n",
    "        transform=True,\n",
    "        train=True,\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_data_set1, batch_size=190, shuffle=False)      \n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for model_select in range(2,3+1):\n",
    "        print('Model: ' + str(model_select))\n",
    "        out_path = 'task/' + 'model' + str(model_select) + '/'\n",
    "        \n",
    "        #Model Selection\n",
    "        pretrain = True\n",
    "        if model_select == 2:\n",
    "            import fknet\n",
    "            model = fknet.resnet50(pretrained=pretrain)\n",
    "            model.fc = nn.Conv2d(832, 190, kernel_size=(6,10), stride=1, padding=0)\n",
    "            model.load_state_dict(torch.load(os.path.join(out_path, 'model.pth')))\n",
    "            model.fc = nn.Flatten()\n",
    "        elif model_select == 3:\n",
    "            fea_size = 800\n",
    "            import fknet\n",
    "            model = fknet.resnet50(pretrained=pretrain)\n",
    "            model.fc = nn.Sequential(nn.Conv2d(832, fea_size, kernel_size=(6,10), stride=1, padding=0),\n",
    "                                     nn.ReLU(),nn.BatchNorm2d(fea_size) )\n",
    "            model2 = nn.Conv2d(fea_size, 190, kernel_size=1, stride=1, padding=0)\n",
    "            model2 = model2.to(device)\n",
    "        model = model.to(device)\n",
    "\n",
    "\n",
    "        if model_select == 2:\n",
    "            test_align_match(train_dataloader,test_dataloader, model, center=True, save=True, batch_size=batch_size)\n",
    "            test_align_match(train_dataloader,test_dataloader, model, center=True, save=True, batch_size=batch_size, sim=2)\n",
    "        elif model_select == 3:\n",
    "            test_align_match(train_dataloader,test_dataloader, model, center=False, save=True, batch_size=batch_size)\n",
    "            test_align_match(train_dataloader,test_dataloader, model, center=False, save=True, batch_size=batch_size, sim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
